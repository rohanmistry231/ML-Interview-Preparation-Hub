# Natural Language Processing Questions

This file contains natural language processing (NLP) questions commonly asked in machine learning interviews at companies like **Google**, **Amazon**, **Meta**, and others. These questions assess your **understanding** of NLP techniques, models, and applications, covering topics like text preprocessing, embeddings, transformers, and evaluation.

Below are the questions with detailed answers, including explanations, technical details, and practical insights for interviews.

---

## Table of Contents

1. [What are the key steps in preprocessing text data for NLP tasks?](#1-what-are-the-key-steps-in-preprocessing-text-data-for-nlp-tasks)
2. [What is tokenization, and why is it important in NLP?](#2-what-is-tokenization-and-why-is-it-important-in-nlp)
3. [Explain the difference between bag-of-words and TF-IDF representations](#3-explain-the-difference-between-bag-of-words-and-tf-idf-representations)
4. [What are word embeddings, and how do they improve NLP models?](#4-what-are-word-embeddings-and-how-do-they-improve-nlp-models)
5. [What is the difference between RNNs and transformers for NLP tasks?](#5-what-is-the-difference-between-rnns-and-transformers-for-nlp-tasks)
6. [Explain the attention mechanism in the context of NLP](#6-explain-the-attention-mechanism-in-the-context-of-nlp)
7. [What is BERT, and how does it differ from traditional word embeddings?](#7-what-is-bert-and-how-does-it-differ-from-traditional-word-embeddings)
8. [How do you evaluate the performance of an NLP model?](#8-how-do-you-evaluate-the-performance-of-an-nlp-model)

---

## 1. What are the key steps in preprocessing text data for NLP tasks?

**Answer**:

Preprocessing text data transforms raw text into a format suitable for NLP models. Key steps:

- **Lowercasing**:
  - Convert text to lowercase (e.g., â€œHelloâ€ â†’ â€œhelloâ€).
  - **Why**: Reduces vocabulary size, ensures consistency.
- **Tokenization**:
  - Split text into tokens (words, subwords, or characters).
  - **Why**: Enables numerical representation.
  - **Tool**: NLTK, spaCy, or model-specific (e.g., BERT tokenizer).
- **Removing Noise**:
  - Strip punctuation, special characters, or URLs.
  - **Why**: Reduces irrelevant features.
- **Stop Word Removal**:
  - Remove common words (e.g., â€œthe,â€ â€œisâ€).
  - **Why**: Focuses on meaningful terms, but context-dependent (e.g., keep for transformers).
- **Stemming/Lemmatization**:
  - Reduce words to root form (e.g., â€œrunningâ€ â†’ â€œrunâ€).
  - **Why**: Normalizes variations, but lemmatization is context-aware.
  - **Tool**: Porter Stemmer, WordNet lemmatizer.
- **Handling Numbers**:
  - Normalize or remove numbers (e.g., â€œ2023â€ â†’ â€œ<NUMBER>â€).
  - **Why**: Generalizes numerical data.
- **N-grams** (optional):
  - Extract multi-word sequences (e.g., â€œmachine learningâ€).
  - **Why**: Captures phrases for some tasks.
- **Encoding**:
  - Convert tokens to IDs for model input (e.g., vocabulary indices).
  - **Why**: Models require numerical input.

**Example**:
- Raw: â€œIâ€™m running to the Store in 2023!â€
- Processed: Tokens = [â€œrunâ€, â€œstoreâ€] (after lowercasing, removing noise, lemmatizing).

**Interview Tips**:
- Tailor steps: â€œDepends on taskâ€”transformers need less cleanup.â€
- Mention tools: â€œspaCy for pipelines, BERT for tokenization.â€
- Be ready to code: â€œShow tokenization in Python.â€

---

## 2. What is tokenization, and why is it important in NLP?

**Answer**:

**Tokenization** is the process of splitting text into smaller units (tokens), such as words, subwords, or characters, to enable numerical processing by NLP models.

- **Types**:
  - **Word**: Split on spaces/punctuation (e.g., â€œI love NLPâ€ â†’ [â€œIâ€, â€œloveâ€, â€œNLPâ€]).
  - **Subword**: Break words into pieces (e.g., â€œplayingâ€ â†’ [â€œplayâ€, â€œ##ingâ€]).
    - Used in BERT, WordPiece, BPE.
  - **Character**: Split into individual characters (e.g., â€œNLPâ€ â†’ [â€œNâ€, â€œLâ€, â€œPâ€]).
- **Why Important**:
  - **Numerical Input**: Models require tokens to map to IDs (e.g., vocabulary).
  - **Granularity**: Captures meaning at right level (e.g., subwords handle rare words).
  - **Context Preservation**: Maintains structure for tasks like translation.
  - **Vocabulary Size**: Balances size vs. coverage (subwords reduce OOV).
- **Challenges**:
  - Ambiguity (e.g., â€œU.S.â€ vs. â€œusâ€).
  - Language-specific rules (e.g., Chinese segmentation).
  - Reversible tokenization for generation.

**Example**:
- Text: â€œunhappinessâ€.
- Subword: [â€œunâ€, â€œ##happinessâ€] (BERT-style).
- Benefit: Handles â€œhappyâ€ and â€œunhappyâ€ consistently.

**Interview Tips**:
- Explain types: â€œSubword is key for modern NLP.â€
- Link to models: â€œBERT uses WordPiece for flexibility.â€
- Be ready to sketch: â€œShow text â†’ tokens â†’ IDs.â€

---

## 3. Explain the difference between bag-of-words and TF-IDF representations

**Answer**:

- **Bag-of-Words (BoW)**:
  - **How**: Represent text as a vector of word counts or presence (binary).
  - **Process**:
    - Build vocabulary (e.g., all unique words).
    - Vectorize: Count occurrences per document (e.g., [2, 0, 1] for words).
  - **Pros**: Simple, captures word frequency.
  - **Cons**: Ignores word order, no semantic meaning, high-dimensional.
  - **Use Case**: Basic text classification (e.g., spam detection).
  - **Example**: â€œcat dogâ€ â†’ [1, 1, 0] (vocab: cat, dog, bird).

- **TF-IDF (Term Frequency-Inverse Document Frequency)**:
  - **How**: Weight words by frequency in document (TF) and rarity across corpus (IDF).
  - **Formula**:
    - TF = `count(word, doc) / len(doc)`.
    - IDF = `log(N / n_t)`, where `N` is docs, `n_t` is docs with word.
    - TF-IDF = `TF * IDF`.
  - **Pros**: Downweights common words (e.g., â€œtheâ€), highlights distinctive terms.
  - **Cons**: Still ignores order, sparse vectors.
  - **Use Case**: Document retrieval, topic modeling.
  - **Example**: â€œcatâ€ in one doc, rare in corpus â†’ high TF-IDF.

- **Key Differences**:
  - **Weighting**: BoW uses raw counts; TF-IDF adjusts for rarity.
  - **Meaning**: BoW treats all words equally; TF-IDF emphasizes uniqueness.
  - **Sparsity**: Both sparse, but TF-IDF reduces noise from frequent words.
  - **ML Context**: BoW for simple models; TF-IDF for search/relevance.

**Example**:
- Docs: â€œcat dogâ€, â€œdog birdâ€.
- BoW: [1, 1, 0], [0, 1, 1].
- TF-IDF: â€œcatâ€ gets higher weight (rare), â€œdogâ€ lower (common).

**Interview Tips**:
- Clarify limits: â€œBoth lose context, unlike embeddings.â€
- Explain IDF: â€œRarity boosts important words.â€
- Be ready to compute: â€œShow TF-IDF for a small corpus.â€

---

## 4. What are word embeddings, and how do they improve NLP models?

**Answer**:

**Word embeddings** are dense, low-dimensional vector representations of words that capture semantic meaning, learned from text data.

- **How They Work**:
  - Map words to vectors (e.g., 300-dim) in a continuous space.
  - Similar words (e.g., â€œkingâ€, â€œqueenâ€) are close in vector space.
  - Learned via models like:
    - **Word2Vec**: Predict word given context (CBOW) or vice versa (Skip-gram).
    - **GloVe**: Factorize co-occurrence matrix.
    - **FastText**: Include subword info for rare words.

- **Why They Improve NLP**:
  - **Semantic Similarity**: Capture relationships (e.g., â€œking - man + woman â‰ˆ queenâ€).
  - **Dimensionality Reduction**: Dense (vs. sparse BoW), reduces parameters.
  - **Generalization**: Handle unseen words via similarity (e.g., FastText subwords).
  - **Transfer Learning**: Pretrained embeddings (e.g., GloVe) boost small datasets.
  - **Contextual Models**: Lead to advanced embeddings (e.g., BERT, contextual).

- **Limitations**:
  - Static embeddings lack context (e.g., â€œbankâ€ as river vs. finance).
  - Compute-intensive to train from scratch.

**Example**:
- Task: Sentiment analysis.
- GloVe: â€œhappyâ€ = [0.1, 0.5, ...], â€œjoyâ€ nearby â†’ model learns positive sentiment.

**Interview Tips**:
- Highlight semantics: â€œEmbeddings encode meaning, not just counts.â€
- Compare: â€œBoW is sparse, embeddings are dense.â€
- Be ready to sketch: â€œShow â€˜kingâ€™, â€˜queenâ€™ in 2D space.â€

---

## 5. What is the difference between RNNs and transformers for NLP tasks?

**Answer**:

- **RNNs (Recurrent Neural Networks)**:
  - **How**: Process sequences sequentially, maintaining a hidden state.
    - Update: `h_t = f(W_h * h_{t-1} + W_x * x_t + b)`.
  - **Variants**: LSTM, GRU mitigate vanishing gradients.
  - **Pros**:
    - Good for short sequences.
    - Memory-efficient for small models.
  - **Cons**:
    - Sequential processing â†’ slow training/inference.
    - Struggles with long-range dependencies (e.g., 100+ tokens).
  - **Use Case**: Early NLP (e.g., sentiment with short texts).
  - **Example**: Predict next word in â€œI loveâ€¦â€ using LSTM.

- **Transformers**:
  - **How**: Process sequences in parallel using self-attention.
    - Attention: `Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V`.
    - Stack encoder/decoder layers for tasks.
  - **Pros**:
    - Captures long-range dependencies (e.g., sentence-wide context).
    - Parallelizable â†’ faster training.
    - Scales to large models (e.g., BERT, GPT).
  - **Cons**:
    - Memory-intensive (quadratic w.r.t. sequence length).
    - Requires large data to train.
  - **Use Case**: Modern NLP (e.g., translation, QA).
  - **Example**: BERT understands â€œbankâ€ context in sentence.

- **Key Differences**:
  - **Processing**: RNNs are sequential; transformers are parallel.
  - **Dependencies**: RNNs struggle with long-range; transformers excel.
  - **Scalability**: Transformers dominate large-scale NLP; RNNs for niche cases.
  - **ML Context**: Transformers replaced RNNs for most tasks (e.g., BERT vs. LSTM).

**Example**:
- RNN: Fails to link â€œheâ€ to â€œJohnâ€ in long text.
- Transformer: Captures link via attention, better accuracy.

**Interview Tips**:
- Emphasize attention: â€œTransformers focus on relevant tokens.â€
- Discuss speed: â€œParallelism makes transformers faster.â€
- Be ready to sketch: â€œShow RNN loop vs. transformer attention.â€

---

## 6. Explain the attention mechanism in the context of NLP

**Answer**:

The **attention mechanism** allows NLP models to focus on relevant parts of a sequence when processing or generating text, improving context understanding.

- **How It Works**:
  - **Scaled Dot-Product Attention**:
    - Input: Query (`Q`), Key (`K`), Value (`V`) vectors for each token.
    - Compute: `Attention = softmax(QK^T/âˆšd_k)V`.
    - Output: Weighted sum of values, emphasizing important tokens.
  - **Multi-Head Attention**:
    - Run multiple attention layers in parallel, concatenate outputs.
    - Captures different relationships (e.g., syntax, semantics).
  - **Self-Attention**:
    - Tokens attend to each other in same sequence (e.g., sentence).
  - **Math**:
    - Similarity: `QK^T` measures token relevance.
    - Scaling: `âˆšd_k` stabilizes gradients.

- **In NLP**:
  - **Transformers**: Core of BERT, GPT, enabling context-aware embeddings.
  - **Tasks**:
    - Translation: Focus on source words for target.
    - QA: Attend to relevant sentence parts.
    - Summarization: Highlight key phrases.
  - **Benefits**:
    - Long-range dependencies (unlike RNNs).
    - Interpretable weights (e.g., see what â€œitâ€ refers to).

**Example**:
- Sentence: â€œThe cat, which is black, sleeps.â€
- Attention: â€œSleepsâ€ attends to â€œcat,â€ not â€œblack,â€ for meaning.

**Interview Tips**:
- Simplify intuition: â€œAttention weighs important words.â€
- Link to transformers: â€œPowers modern NLP models.â€
- Be ready to derive: â€œShow attention matrix calculation.â€

---

## 7. What is BERT, and how does it differ from traditional word embeddings?

**Answer**:

**BERT** (Bidirectional Encoder Representations from Transformers) is a pretrained transformer model that generates contextual word embeddings for NLP tasks.

- **How BERT Works**:
  - **Architecture**: Stack of transformer encoders (e.g., 12 layers in BERT-base).
  - **Pretraining**:
    - **Masked Language Model (MLM)**: Predict masked words (15% of tokens).
    - **Next Sentence Prediction (NSP)**: Predict if sentences follow each other.
  - **Fine-Tuning**: Adapt to tasks (e.g., classification, QA) with task-specific data.
  - **Output**: Contextual embeddings for each token, varying by sentence.

- **Traditional Word Embeddings**:
  - **Examples**: Word2Vec, GloVe, FastText.
  - **How**: Static vectors per word, trained on co-occurrence or prediction.
  - **Properties**: Same vector for â€œbankâ€ in all contexts.

- **Key Differences**:
  - **Contextuality**:
    - BERT: Dynamic embeddings (e.g., â€œbankâ€ differs in â€œriver bankâ€ vs. â€œbank accountâ€).
    - Traditional: Fixed embeddings, no context.
  - **Directionality**:
    - BERT: Bidirectional, considers full sentence.
    - Traditional: Unidirectional or context-free.
  - **Training**:
    - BERT: Pretrained on large corpus, fine-tuned.
    - Traditional: Trained once, used as-is or lightly adapted.
  - **Performance**:
    - BERT: Superior for tasks like QA, sentiment (context matters).
    - Traditional: Simpler, faster for basic tasks.
  - **ML Context**: BERT powers modern NLP; traditional embeddings for lightweight models.

**Example**:
- Sentence: â€œI bank money.â€
- BERT: â€œbankâ€ embedding reflects finance.
- GloVe: Same â€œbankâ€ vector for all uses.

**Interview Tips**:
- Highlight context: â€œBERT adapts to sentence meaning.â€
- Compare size: â€œBERT is heavy, GloVe is light.â€
- Be ready to sketch: â€œShow BERTâ€™s transformer layers.â€

---

## 8. How do you evaluate the performance of an NLP model?

**Answer**:

Evaluating NLP models depends on the task, using metrics to measure accuracy, robustness, and generalization:

- **Classification Tasks** (e.g., sentiment analysis):
  - **Accuracy**: Fraction of correct predictions.
  - **Precision/Recall/F1**:
    - Precision: `TP / (TP + FP)` (correct positives).
    - Recall: `TP / (TP + FN)` (captured positives).
    - F1: `2 * (P * R) / (P + R)` (harmonic mean).
  - **Use**: F1 for imbalanced data (e.g., spam detection).
- **Sequence Labeling** (e.g., NER):
  - **Token-Level F1**: Evaluate per token (e.g., â€œB-PERâ€, â€œI-PERâ€).
  - **Entity-Level F1**: Match full entities (e.g., â€œJohn Doeâ€).
  - **Use**: Entity F1 for strict matching.
- **Generation Tasks** (e.g., translation, summarization):
  - **BLEU**: Measures n-gram overlap with reference.
  - **ROUGE**: Recall-oriented for summaries (e.g., ROUGE-L for longest sequence).
  - **METEOR**: Considers synonyms, stemming.
  - **Use**: BLEU for translation, ROUGE for summarization.
- **Question Answering**:
  - **Exact Match (EM)**: Full answer match.
  - **F1 Score**: Overlap of answer tokens.
  - **Use**: EM for strict evaluation, F1 for partial credit.
- **Embedding-Based** (e.g., similarity):
  - **Cosine Similarity**: Measure vector alignment.
  - **Correlation**: Compare to human judgments.
  - **Use**: Evaluate semantic search.
- **Human Evaluation**:
  - Subjective scoring for fluency, coherence (e.g., chatbot responses).
  - **Use**: When metrics fail (e.g., creative tasks).
- **General Practices**:
  - **Cross-Validation**: Ensure robustness.
  - **Error Analysis**: Inspect mispredictions (e.g., confusion matrix).
  - **Domain-Specific**: Align metrics with goals (e.g., latency for real-time).

**Example**:
- Task: Sentiment classification.
- Metrics: F1 = 0.85 (handles imbalance), accuracy = 0.88.
- Analysis: Check false positives for improvement.

**Interview Tips**:
- Match metric to task: â€œF1 for classification, BLEU for translation.â€
- Discuss limits: â€œBLEU misses fluency, needs human eval.â€
- Be ready to compute: â€œShow F1 formula.â€

---

## Notes

- **Focus**: Answers cover NLP fundamentals and advanced models, ideal for ML interviews.
- **Clarity**: Explanations are structured for verbal delivery, with examples and trade-offs.
- **Depth**: Includes technical details (e.g., attention math, BERT pretraining) and practical tips (e.g., preprocessing pipelines).
- **Consistency**: Matches the style of previous files for a cohesive repository.

For deeper practice, implement NLP models (see [ML Coding](ml-coding.md)) or explore [Deep Learning](deep-learning.md) for transformer foundations. ğŸš€

---

**Next Steps**: Build on these skills with [Statistics & Probability](statistics-probability.md) for NLP evaluation metrics or revisit [Production MLOps](production-mlops.md) for deploying NLP models! ğŸŒŸ